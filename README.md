# selectTop100
分析：

大文件分割：
首先考虑到每个URL长度不一致，这里把每个URL长度规定在1字节到30字节的范围内，即每一条URL条目大小为1B~30B，这里假设每个URL存一行，如果每个	URL记录都是以追加写log形式写到100GB文件中，即URL和URL之间用逗号或空格隔开，则先按逗号或空格隔开，每次读取一条数据

如果把100GB的URL文件按均匀大小的500个文件（每个文件并不是完全等大小，要考虑内部URL数据对齐问题，即一个URL数据不能被分割在两个文件中），每个文件大约为200MB，这样一个拥有1GB内存的主机可以对一个小文件读取到内存进行排序处理

分割方式：
1、用Hash函数对URL进行划分，相同的URL会尽可能在同一个文件中，但是在上亿条数据量的情况下，考虑到Hash冲突以及数据量大小，相同的URL也可能会被分到不同的1~n个小文件中，最后还是要进行归并操作
2、按大小分割成1~n个小文件，这种方法简单快速，分割后的每个文件大小均匀，本算法采用的是按大小进行分割

关于排序问题，这里有两种情况：
1、对每个200MB的1~n个小文件根据每个URL的出现次数进行排序，在整个文件遍历一遍后，1~n个小文件内部都没有重复的URL，并且按照URL出现次数进行排序；
2、对每个200MB小文件根据URL字符串值进行排序，对相同的URL出现次数进行累加，在排序完成后小文件内部没有重复URL，这样排序后的文件是以<URL，count>键值对的形式保存在文件中，可以减小中间结果的大小，本算法采用这种方式进行排序

查找url出现次数的top100问题
在每个小文件进行排序后，多所有小文件进行归并排序，把不同文件里相同的url值合并，并把count累加，再存到

本算法的小文件都是以<url,count>键值对的形式存储，排序时也是以url值进行排序，这样在生成总体有序大文件时，多路归并排序更方便，因为查找相同的url只需要比较每个文件的最前面<url,count>对的url值就行

整个过程的步骤：
1、大文件（100GB）分割成n个小文件，每个小文件大概200MB
2、依次对n个.txt小文件进行内部排序，排序后文件内条目格式为<url,count>，且1~n个小文件内没有重复的url，count为对应相同url出现次数
3、对n个有序的小文件进行多路归并排序，生成总体有序的result.txt文件，每条数据以<url,count>格式存储，count为每个url出现次数的最终值
4、对result.txt文件进行一遍遍历，用大小设定为100的multimap<int,string>保存出现次数为TOP100的<url,count>，然后输入到top100.txt文件中（这一步也可以在多路归并的时候进行，减少一次整体遍历，但是算法采用归并后再遍历，为一般做法）

